{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfe691e-4401-41de-9722-f1dbe749bd44",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Machine Learning Failures\"\n",
    "description: \"what are some common instances seen in society?\"\n",
    "author: \"Angelica Hu\"\n",
    "date: \"11/03/2025\"\n",
    "categories:\n",
    "- LLMs\n",
    "- ethics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce2daa-92fe-4bda-8f25-0fe4c30f1d02",
   "metadata": {},
   "source": [
    "<img src = \"machine.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16221fd4-aeab-4fa4-a0ca-8ee71d298c5e",
   "metadata": {},
   "source": [
    "While I was reading more of \"Unmasking AI,\" by Dr. Joy Buolamwini, she introduced this idea of machinese learning failures that were related to real life scenarios about the coded gaze. For example, in Chapter 6, Buolamwini talks about how a lack of accurate facial recognition softwares became responsible for wrongful accusations in prision, women who were wrongly accused as prositutes, and costumers being accused for shop lifting. Instances like these made me curious, what are some examples of harmful machine learning failures in our society and why does the general public often choose to ingore the many ethical concerns associated with AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765c2a2-5a4b-48cc-9ec8-675c351e3ac4",
   "metadata": {},
   "source": [
    "I found two interesting reads regarding these concerns: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add9058-53f9-4b7d-bcb5-7082737fd90f",
   "metadata": {},
   "source": [
    "* https://www.ethics.harvard.edu/blog/post-8-abyss-examining-ai-failures-and-lessons-learned#:~:text=In%20addition%2C%20in%202016%2C%20the,light%2C%20colliding%20with%20another%20vehicle.\n",
    "\n",
    "  \n",
    "* https://medium.com/@techwithpraisejames/when-ai-gets-it-wrong-real-stories-of-ai-fails-and-what-we-can-learn-26972dff8b86"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101ed3e-ed24-4128-83a3-e135dcd013f6",
   "metadata": {},
   "source": [
    "Two main cases regarding big companies here to focus on are Amazon and Uber. Essentially, back in 2014 (we know AI has become much more advanced since then), Amazon implemented technology and automation in their hiring process very subtly. However, people realized this was the case when the algorithm had a substantial bias toward hiring men, creating gender discrimination built on past resumes. As for Uber, when partnering with Volvo to introduce an idea of self-driving cars, their system failed to recognize a pedestrian crossing the street on a bike which resulted in a fatal accident. These are only a few of too many incidents we see where the conseuqences of automation, AI, and technology are often helpful in theory, but ultimetly backfire if we aren't careful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5bb49-1672-4572-bf1a-2c540f920495",
   "metadata": {},
   "source": [
    "In the medium post, a similar intellect suggests that after looking at these situations, we must consider responsible AI development to get AI right. However, is this the case with current companies focused on AI development? As we know, OpenAI tends to be extremely secretive about their training practices and their developing processes, so why is it that we all tend to so easily trust this technology? Is it out of convinence and because it is so easily accessible? Food for thought..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c615f3-753e-4afd-b4e5-7fca0513b960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
