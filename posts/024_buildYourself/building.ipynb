{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97bb53e-344d-4ab8-8360-b18873cde703",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Let's Build ChatGPT\"\n",
    "description: \"can chat build itself?\"\n",
    "author: \"Angelica Hu\"\n",
    "date: \"09/16/2025\"\n",
    "categories:\n",
    "- Prompting\n",
    "- LLMs\n",
    "- Logic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d735c8e3-29d3-40ca-9d6c-f8d512952533",
   "metadata": {},
   "source": [
    "<img src=\"build.jpg\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb2007-c501-462e-bba2-b7d3eb7b4a89",
   "metadata": {},
   "source": [
    "Yesterday I experimented with having chatGPT explain its origins, or how it was made. It could explain its process and inner workings in simple terms, but does it have the tehcnical advancement to write the code to create an LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89f438-1945-4cda-97a2-bd4e55c1bf5c",
   "metadata": {},
   "source": [
    "### USER\n",
    "Write the code to create an LLM.\n",
    "\n",
    "### ASSISTANT\n",
    "Creating a language model (LLM) from scratch involves several complex steps, including data collection, preprocessing, model selection, training, and evaluation. Given the complexity and scale typically involved in creating large language models, it's common to use existing frameworks and libraries such as PyTorch or TensorFlow, often leveraging pre-trained models and fine-tuning them on specific tasks.\n",
    "\n",
    "Here's a general outline and some example code to get started with creating and fine-tuning a language model using Hugging Face Transformers library, which simplifies the process significantly:\n",
    "\n",
    "### Step 1: Set Up Your Environment\n",
    "\n",
    "Ensure you have Python, TensorFlow or PyTorch, and relevant libraries installed. You can install the necessary libraries using pip:\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets\n",
    "```\n",
    "\n",
    "### Step 2: Load and Preprocess Data\n",
    "\n",
    "For language modeling, datasets like WikiText, Common Crawl, etc., can be used. Hereâ€™s how you might load a dataset:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-2', split='train')\n",
    "\n",
    "# Preprocess the data as required\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "\n",
    "# Initialize a tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "```\n",
    "\n",
    "### Step 3: Setup the Model\n",
    "\n",
    "Load a pre-trained model and fine-tune it:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "```\n",
    "\n",
    "### Step 4: Train the Model\n",
    "\n",
    "Setup the training parameters and train the model:\n",
    "\n",
    "```python\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    ")\n",
    "\n",
    "# Define a Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "### Step 5: Evaluate the Model\n",
    "\n",
    "Evaluate to check the model's performance:\n",
    "\n",
    "```python\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n",
    "```\n",
    "\n",
    "### Step 6: Save the Model\n",
    "\n",
    "After training and evaluation, save your model for future use:\n",
    "\n",
    "```python\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./fine-tuned-gpt2')\n",
    "tokenizer.save_pretrained('./fine-tuned-gpt2')\n",
    "```\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- **Compute Resources:** Training large models require substantial compute resources, often provided by GPUs or TPUs.\n",
    "- **Ethical Considerations:** Ensure your dataset and usage adhere to ethical guidelines and consider the implications of deploying such models.\n",
    "- **Hyperparameters:** Optimal hyperparameters can vary substantially based on the task and dataset, requiring experimentation to fine-tune.\n",
    "\n",
    "This example assumes basic usage and a general-purpose task. Specific use-cases or more complex requirements might involve additional steps or configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15974b5-fb75-44f2-a59a-75831b56dbfa",
   "metadata": {},
   "source": [
    "I attemped to give it a shot and upload this code into my jupyternotebook, but ran into many errors with having a restricted API and environment. But by looking at this code, chatGPT did a good job providing the basics mechanics that runs an LLM that is similar to the methodology we discussed in class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618ed8f-a8c1-41f5-b64e-eacd9c0a23fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
