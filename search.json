[
  {
    "objectID": "troubleshoot.html",
    "href": "troubleshoot.html",
    "title": "My Explorations With LLMs",
    "section": "",
    "text": "!grep -Rn --include=\"*.qmd\" \"about:\" .\n\n./.ipynb_checkpoints/about-checkpoint.qmd:4:about:\n./about.qmd:4:about:\n\n\n\n!nl -ba about.qmd | sed -n '1,40p'\n!nl -ba ./.ipynb_checkpoints/about-checkpoint.qmd | sed -n '1,40p'\n\n     1  ---\n     2  title: \"About\"\n     3  image: profile.jpg\n     4  about:\n     5    template: jolla\n     6  ---\n     7  \n     8  Hi! My name is Angelica and I'm a current sophomore at the University of Pennsylvania studying Health & Societies, concentrating in Healthcare Markets and Finance. I'm also minoring in Neuroscience & Healthcare Management, and Data Science and Analytics.\n     9  \n    10  Angelica Hu\n     1  ---\n     2  title: About\n     3  image: profile.jpg\n     4  about:\n     5    template: jolla\n     6  ---\n     7  \n     8  About this blog\n\n\n\n!rm -rf ./.ipynb_checkpoints\n\n\n!printf \".ipynb_checkpoints/\\n\" &gt; .quartoignore\n!printf \"\\n.ipynb_checkpoints/\\n\" &gt;&gt; .gitignore\n\n\n!rm -rf ./_site\n!rm -rf ./.quarto\n\n\n!grep -Rn --include=\"*.qmd\" --include=\"*.yml\" --include=\"*.yaml\" '^about:\\s\\S' . || true\n\n\n!grep -RnP '\\t' --include=\"*.qmd\" --include=\"*.yml\" --include=\"*.yaml\" . || true\n\n\n!git add -A\n!git commit -m \"Clean checkpoints, ignore them; clear Quarto cache; confirm about: mapping\"\n!git push\n\n[main 67675f1] Clean checkpoints, ignore them; clear Quarto cache; confirm about: mapping\n 4 files changed, 58 insertions(+), 1 deletion(-)\n create mode 100644 .quartoignore\n create mode 100644 troubleshoot.ipynb\nEnumerating objects: 9, done.\nCounting objects: 100% (9/9), done.\nDelta compression using up to 16 threads\nCompressing objects: 100% (5/5), done.\nWriting objects: 100% (6/6), 1.04 KiB | 355.00 KiB/s, done.\nTotal 6 (delta 3), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (3/3), completed with 3 local objects.\nTo https://github.com/angelicahu10/comm4190_F25_Using_LLMs_Blog.git\n   17eeece..67675f1  main -&gt; main"
  },
  {
    "objectID": "technical.html",
    "href": "technical.html",
    "title": "1. Text generation",
    "section": "",
    "text": "from openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\nclient = OpenAI()\n# to get a response must end on a user reponse\nmessages = [\n    {\"role\": \"system\", \n     \"content\": '''\n                   You are a poetic assistant, skilled in explaining complex programming concepts \n                   with creative flair.\n                '''},\n    \n    {\"role\": \"user\", \n     \"content\": '''\n                 Compose a poem that explains why the sky is blue in eight lines.\n                '''}\n     \n  ]\ncompletion = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=messages\n)\n\nresponse = completion.choices[0].message.content\nprint(response)\n\nIn daylight's dance, the sky dons hues of blue,  \nAs sunlight weaves through air, a magic true.  \nRayleigh's scattering acts as light's gentle sieve,  \nShort waves of blue, more wily, twist and live.  \n\nThe azure blooms as longer waves drift away,  \nLeaving the sky in a cerulean display.  \nThus, in daylight's tapestry, blue shines with grace,  \nA secret whispered by the sun's embrace."
  },
  {
    "objectID": "technical.html#image-to-text",
    "href": "technical.html#image-to-text",
    "title": "1. Text generation",
    "section": "Image-to-text",
    "text": "Image-to-text\n\n\nimg_path = \"/Commjhub/jupyterhub/home/angelicahu10/comm4190_F25/comm4190_F25_Using_LLMs_Blog/posts/003_professor_image/test_image.jpg\"\nbase64_image = encode_image(img_path)\nprompt = 'Describe this image'\nmax_tokens = 100\n\n\nAs part of the user message you can include the image along with a text prompt to direct how the model should analyze it.\n\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(model.id)\n\ngpt-4-0613\ngpt-4\ngpt-3.5-turbo\ngpt-audio\ngpt-5-nano\ngpt-audio-2025-08-28\ngpt-realtime\ngpt-realtime-2025-08-28\ndavinci-002\nbabbage-002\ngpt-3.5-turbo-instruct\ngpt-3.5-turbo-instruct-0914\ndall-e-3\ndall-e-2\ngpt-4-1106-preview\ngpt-3.5-turbo-1106\ntts-1-hd\ntts-1-1106\ntts-1-hd-1106\ntext-embedding-3-small\ntext-embedding-3-large\ngpt-4-0125-preview\ngpt-4-turbo-preview\ngpt-3.5-turbo-0125\ngpt-4-turbo\ngpt-4-turbo-2024-04-09\ngpt-4o\ngpt-4o-2024-05-13\ngpt-4o-mini-2024-07-18\ngpt-4o-mini\ngpt-4o-2024-08-06\nchatgpt-4o-latest\no1-mini-2024-09-12\no1-mini\ngpt-4o-realtime-preview-2024-10-01\ngpt-4o-audio-preview-2024-10-01\ngpt-4o-audio-preview\ngpt-4o-realtime-preview\nomni-moderation-latest\nomni-moderation-2024-09-26\ngpt-4o-realtime-preview-2024-12-17\ngpt-4o-audio-preview-2024-12-17\ngpt-4o-mini-realtime-preview-2024-12-17\ngpt-4o-mini-audio-preview-2024-12-17\no1-2024-12-17\no1\ngpt-4o-mini-realtime-preview\ngpt-4o-mini-audio-preview\ncomputer-use-preview\no3-mini\no3-mini-2025-01-31\ngpt-4o-2024-11-20\ncomputer-use-preview-2025-03-11\ngpt-4o-search-preview-2025-03-11\ngpt-4o-search-preview\ngpt-4o-mini-search-preview-2025-03-11\ngpt-4o-mini-search-preview\ngpt-4o-transcribe\ngpt-4o-mini-transcribe\no1-pro-2025-03-19\no1-pro\ngpt-4o-mini-tts\no3-2025-04-16\no4-mini-2025-04-16\no3\no4-mini\ngpt-4.1-2025-04-14\ngpt-4.1\ngpt-4.1-mini-2025-04-14\ngpt-4.1-mini\ngpt-4.1-nano-2025-04-14\ngpt-4.1-nano\ngpt-image-1\ncodex-mini-latest\no3-pro\ngpt-4o-realtime-preview-2025-06-03\ngpt-4o-audio-preview-2025-06-03\no3-pro-2025-06-10\no4-mini-deep-research\no3-deep-research\no3-deep-research-2025-06-26\no4-mini-deep-research-2025-06-26\ngpt-5-chat-latest\ngpt-5-2025-08-07\ngpt-5\ngpt-5-mini-2025-08-07\ngpt-5-mini\ngpt-5-nano-2025-08-07\ngpt-3.5-turbo-16k\ntts-1\nwhisper-1\ntext-embedding-ada-002\n\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n      max_tokens=max_tokens,\n    )\n\n\n---------------------------------------------------------------------------\nBadRequestError                           Traceback (most recent call last)\nCell In[112], line 1\n----&gt; 1 response = client.chat.completions.create(\n      2     model=\"o1\",\n      3     messages=[\n      4         {\n      5           \"role\": \"user\",\n      6           \"content\": [\n      7             {\"type\": \"text\", \"text\": prompt},\n      8             {\n      9               \"type\": \"image_url\",\n     10               \"image_url\": {\n     11                 \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n     12               },\n     13             },\n     14           ],\n     15         }\n     16       ],\n     17       max_tokens=max_tokens,\n     18     )\n\nFile /opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/openai/_utils/_utils.py:279, in required_args.&lt;locals&gt;.inner.&lt;locals&gt;.wrapper(*args, **kwargs)\n    277             msg = f\"Missing required argument: {quote(missing[0])}\"\n    278     raise TypeError(msg)\n--&gt; 279 return func(*args, **kwargs)\n\nFile /opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/openai/resources/chat/completions.py:863, in Completions.create(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\n    821 @required_args([\"messages\", \"model\"], [\"messages\", \"model\", \"stream\"])\n    822 def create(\n    823     self,\n   (...)\n    860     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n    861 ) -&gt; ChatCompletion | Stream[ChatCompletionChunk]:\n    862     validate_response_format(response_format)\n--&gt; 863     return self._post(\n    864         \"/chat/completions\",\n    865         body=maybe_transform(\n    866             {\n    867                 \"messages\": messages,\n    868                 \"model\": model,\n    869                 \"audio\": audio,\n    870                 \"frequency_penalty\": frequency_penalty,\n    871                 \"function_call\": function_call,\n    872                 \"functions\": functions,\n    873                 \"logit_bias\": logit_bias,\n    874                 \"logprobs\": logprobs,\n    875                 \"max_completion_tokens\": max_completion_tokens,\n    876                 \"max_tokens\": max_tokens,\n    877                 \"metadata\": metadata,\n    878                 \"modalities\": modalities,\n    879                 \"n\": n,\n    880                 \"parallel_tool_calls\": parallel_tool_calls,\n    881                 \"prediction\": prediction,\n    882                 \"presence_penalty\": presence_penalty,\n    883                 \"reasoning_effort\": reasoning_effort,\n    884                 \"response_format\": response_format,\n    885                 \"seed\": seed,\n    886                 \"service_tier\": service_tier,\n    887                 \"stop\": stop,\n    888                 \"store\": store,\n    889                 \"stream\": stream,\n    890                 \"stream_options\": stream_options,\n    891                 \"temperature\": temperature,\n    892                 \"tool_choice\": tool_choice,\n    893                 \"tools\": tools,\n    894                 \"top_logprobs\": top_logprobs,\n    895                 \"top_p\": top_p,\n    896                 \"user\": user,\n    897             },\n    898             completion_create_params.CompletionCreateParams,\n    899         ),\n    900         options=make_request_options(\n    901             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n    902         ),\n    903         cast_to=ChatCompletion,\n    904         stream=stream or False,\n    905         stream_cls=Stream[ChatCompletionChunk],\n    906     )\n\nFile /opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/openai/_base_client.py:1290, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\n   1276 def post(\n   1277     self,\n   1278     path: str,\n   (...)\n   1285     stream_cls: type[_StreamT] | None = None,\n   1286 ) -&gt; ResponseT | _StreamT:\n   1287     opts = FinalRequestOptions.construct(\n   1288         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1289     )\n-&gt; 1290     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n\nFile /opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/openai/_base_client.py:967, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    964 else:\n    965     retries_taken = 0\n--&gt; 967 return self._request(\n    968     cast_to=cast_to,\n    969     options=options,\n    970     stream=stream,\n    971     stream_cls=stream_cls,\n    972     retries_taken=retries_taken,\n    973 )\n\nFile /opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/openai/_base_client.py:1071, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1068         err.response.read()\n   1070     log.debug(\"Re-raising status error\")\n-&gt; 1071     raise self._make_status_error_from_response(err.response) from None\n   1073 return self._process_response(\n   1074     cast_to=cast_to,\n   1075     options=options,\n   (...)\n   1079     retries_taken=retries_taken,\n   1080 )\n\nBadRequestError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n\n\n\n\nThe response object contains a lot of information returned by the API call\n\n\nresponse\n\nChatCompletion(id='chatcmpl-CDwAqoQDkta7HqdsyDO6o36984aYB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The image shows a person wearing a neck brace and looking at the camera. They have some visible injuries on their face, including cuts and bruises. Their right hand is bandaged, and they are holding it up with three fingers extended. The background appears to be indoors with a door and some light coming through.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1757437920, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_cbf1785567', usage=CompletionUsage(completion_tokens=63, prompt_tokens=775, total_tokens=838, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n\n\n\nYou can pick out just the message\n\n\nprint(response.choices[0].message.content)\n\nThe image shows a person wearing a neck brace and looking at the camera. They have some visible injuries on their face, including cuts and bruises. Their right hand is bandaged, and they are holding it up with three fingers extended. The background appears to be indoors with a door and some light coming through."
  },
  {
    "objectID": "technical.html#text-to-image",
    "href": "technical.html#text-to-image",
    "title": "1. Text generation",
    "section": "Text to image",
    "text": "Text to image\n\nVarious models will also generate an image based on a text description passed in the API call\n\n\nimage_prompt1 = '''\n            create an image of a man holding his hand up'''\n\n\n\n\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt1,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n\n\nThe generated image will be temporary available on a cloud server\n\n\nimage_url\n\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-PIHE3jWQagNqeNkB0RO91xNW.png?st=2025-09-09T16%3A12%3A46Z&se=2025-09-09T18%3A12%3A46Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=52f8f7b3-ca8d-4b21-9807-8b9df114d84c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-09-09T17%3A12%3A46Z&ske=2025-09-10T17%3A12%3A46Z&sks=b&skv=2024-08-04&sig=D8wVb5cXDk9%2Bptx0fCRrDbMj3B/aJNGZnRvN9zdeohs%3D'\n\n\n\nBut most often you will want to download it as part of your script\n\n\ngenerated_image = requests.get(image_url)\n\nwith open('generated_img.png', 'wb') as out:\n    out.write(generated_image.content)"
  },
  {
    "objectID": "posts/004_origins_ChatGPT/origins_ChatGPT.html#video-notes",
    "href": "posts/004_origins_ChatGPT/origins_ChatGPT.html#video-notes",
    "title": "The Birth of ChatGPT",
    "section": "Video Notes",
    "text": "Video Notes\n\nsiloed networks model intuition only, not reasoning -&gt; must train machines to talk\nlearned sequences were generalized\nsystem spatially organizes words based on meaning of words\nfeeding the output back into the input with a short prompt, following a pathway of thought\nrecurrent neural networks that become much larger systems to increase percision and predictability\ntransformers = takes context around it to change its meanings\nzero shot learning\nmaking the network contiues to make it speak less giberrish and increased performance\nin context learning\ncore in weight learning = during training\nin context learning = in use or inference\nLLMs should not be considered a chatbot or word generator, rtaher a kernal process of an operating system\nneural networks allow deep learning and for AI to make connections and draw predictions\n\n\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\n\n\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\n\n client = OpenAI()\n\n\nprompt1 = '''\n\nTell me about how you were created, or the origins of ChatGPT!\n\n'''\n\nprompt2 = '''\n\nTell me about how you were created, or the origins of ChatGPT in simple terms, to someone who is not familar with technical terms.\n\n'''\n\n\n messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ]\n\n\ndef generate_story(prompt1, model=\"gpt-4o\"):\n\n    completion = client.chat.completions.create(\n      model=model,\n      messages=[\n        {\"role\": \"system\", \n         \"content\": \"You are a helpful AI assistant.\"},\n        {\"role\": \"user\", \n         \"content\": prompt1}\n      ]\n    )\n    \n    response = completion.choices[0].message.content\n    return response\n\n\ngenerate_story(prompt1, model=\"gpt-4o\")\n\n'ChatGPT, like many AI language models, is based on the GPT (Generative Pre-trained Transformer) architecture, developed by OpenAI. The creation of GPT models involves several key components and milestones:\\n\\n1. **Transformer Architecture**: The foundation of GPT models is the transformer architecture, which was introduced in the 2017 paper \"Attention is All You Need\" by Vaswani et al. This architecture uses attention mechanisms that allow the model to weigh the importance of different words in a sentence when making predictions, which enables more efficient processing of large datasets and capture of contextual relationships.\\n\\n2. **Pre-training and Fine-tuning**: GPT models are developed through a two-step process: pre-training and fine-tuning. In the pre-training phase, the model is trained on a large corpus of text from the internet to predict the next word in a sentence, learning grammar, facts about the world, and some reasoning abilities. During fine-tuning, the model is adjusted for specific tasks (e.g., conversation), using a narrower dataset with human annotations to align the model’s behavior with specific goals.\\n\\n3. **GPT Series**: The evolution of GPT models began with GPT-1, released in 2018, which had 117 million parameters. OpenAI then released GPT-2 in 2019, notable for its size (1.5 billion parameters) and improved language understanding capabilities. GPT-3 followed in 2020, making a significant leap with 175 billion parameters, allowing for even more nuanced text generation and greater performance across a range of tasks.\\n\\n4. **ChatGPT**: Building on the advances of GPT-3 and its predecessors, ChatGPT was developed to generate interactive, conversational text. OpenAI applied reinforcement learning from human feedback (RLHF) to improve the model\\'s ability to follow instructions and provide meaningful, context-rich responses in a conversational format.\\n\\n5. **Iterative Improvements and Public Use**: OpenAI has continued to iterate on GPT models, incorporating feedback from public use to improve safety, reduce bias, and enhance capabilities. Subsequent updates and versions have further refined the system’s performance and user experience.\\n\\nOverall, the creation of ChatGPT involved advances in machine learning, natural language processing, and AI ethics, with the aim of building powerful and adaptable language models for various applications.'\n\n\n\n messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt2\n            }\n        ]\n\n\ndef generate_story(prompt2, model=\"gpt-4o\"):\n\n    completion = client.chat.completions.create(\n      model=model,\n      messages=[\n        {\"role\": \"system\", \n         \"content\": \"You are a helpful AI assistant.\"},\n        {\"role\": \"user\", \n         \"content\": prompt2}\n      ]\n    )\n    \n    response = completion.choices[0].message.content\n    return response\n\n\ngenerate_story(prompt2, model=\"gpt-4o\")\n\n'Sure! ChatGPT is a type of computer program designed to have conversations with people, like a virtual assistant.\\n\\nThe origins of ChatGPT can be traced back to the development of AI, or artificial intelligence, which is the idea of creating machines that can perform tasks that usually require human intelligence. To make ChatGPT, researchers and engineers used advanced technology and a lot of data to teach the program how to understand and respond to human language.\\n\\nThey created a special kind of program called a \"language model.\" This model was trained on a vast amount of text—from books, websites, and other sources—so it could learn how people use language. In simple terms, it learned from examples of human language how to predict and generate text that makes sense in a conversation.\\n\\nThe goal was to create a helpful tool that could understand questions and provide useful answers, like having a conversation with a knowledgeable friend. Over time, the technology has improved, making ChatGPT better at understanding context and providing more accurate responses.\\n\\nThe team behind ChatGPT is continually working on refining and improving it, so it gets better at helping people with their questions and needs over time.'"
  },
  {
    "objectID": "posts/002_wolfram_article/wolfram.html",
    "href": "posts/002_wolfram_article/wolfram.html",
    "title": "The Wolfram Article",
    "section": "",
    "text": "The way that LLMs basically create output is with “reasonable continuation,” looking at the response with the highest probability. I think it’s super interesting to learn that ChatGpt or other models are basically just making their best guesses at what makes sense, using lots of randomness to come up with non-flat responses. There’s an infinite combination of possible sequences with no correct answer. It was super cool to see how the next word that is outputted is based on the previous word, which then becomes sequences. I found it much easier to understand this complex process when breaking it down from letters, to words, to sentences.\n\nI tried to follow the coding in my Jupyter notebook but learned that the code in the article followed Mathematica/Wolfram syntax instead of Python syntax. I’m interested to learn what differentiates the two, and how to convert them to work with certin models.\nI was also surprised to learn that there are 40,000 reasonably common words in English. While I know English is a vast language, I had never given it much thought how many words I typically use on a daily basis or the words used in conversation!"
  },
  {
    "objectID": "posts/002_wolfram_article/wolfram.html#section",
    "href": "posts/002_wolfram_article/wolfram.html#section",
    "title": "The Wolfram Article",
    "section": "",
    "text": "The way that LLMs basically create output is with “reasonable continuation,” looking at the response with the highest probability. I think it’s super interesting to learn that ChatGpt or other models are basically just making their best guesses at what makes sense, using lots of randomness to come up with non-flat responses. There’s an infinite combination of possible sequences with no correct answer. It was super cool to see how the next word that is outputted is based on the previous word, which then becomes sequences. I found it much easier to understand this complex process when breaking it down from letters, to words, to sentences.\n\nI tried to follow the coding in my Jupyter notebook but learned that the code in the article followed Mathematica/Wolfram syntax instead of Python syntax. I’m interested to learn what differentiates the two, and how to convert them to work with certin models.\nI was also surprised to learn that there are 40,000 reasonably common words in English. While I know English is a vast language, I had never given it much thought how many words I typically use on a daily basis or the words used in conversation!"
  },
  {
    "objectID": "posts/001_first_post/personal_info.html",
    "href": "posts/001_first_post/personal_info.html",
    "title": "My First Blog Post!",
    "section": "",
    "text": "Hello! My name is Angelica and this first blog post is going to be getting to know a little bit about me while I explore how to use this platform! My experience with data science/coding comes from PHYS1100: Foundations of Data Science, which I took last semester as well as some other coding experience I did in high school."
  },
  {
    "objectID": "posts/001_first_post/personal_info.html#hello",
    "href": "posts/001_first_post/personal_info.html#hello",
    "title": "My First Blog Post!",
    "section": "",
    "text": "Hello! My name is Angelica and this first blog post is going to be getting to know a little bit about me while I explore how to use this platform! My experience with data science/coding comes from PHYS1100: Foundations of Data Science, which I took last semester as well as some other coding experience I did in high school."
  },
  {
    "objectID": "posts/001_first_post/personal_info.html#my-family",
    "href": "posts/001_first_post/personal_info.html#my-family",
    "title": "My First Blog Post!",
    "section": "My Family",
    "text": "My Family\nMy parents are from China, but me and my two older sisters were born here which makes us second generation. My older sisters both live in New York City now after graduating from college. This photo is from June from my middle sister’s college graduation in Evanston, Illinois. I loveee Chicago!\nWe also have a dog named Minnie! She’s a goldendoodle and turning 11 this year. I miss her a lot!"
  },
  {
    "objectID": "posts/001_first_post/personal_info.html#academics",
    "href": "posts/001_first_post/personal_info.html#academics",
    "title": "My First Blog Post!",
    "section": "Academics",
    "text": "Academics\nHere at Penn, I’m studying Health and Societies with a concentration in Healthcare Markets and Finance and a minor in Data Science and Analytics. I hope to explore the healthcare system and work towards fixing health disparities and improve access and quality, perhaps through healthcare data analytics!"
  },
  {
    "objectID": "posts/001_first_post/personal_info.html#interests",
    "href": "posts/001_first_post/personal_info.html#interests",
    "title": "My First Blog Post!",
    "section": "Interests",
    "text": "Interests\nI love competitive swimming as I’ve swam my whole life. I also am a big foodie and love trying new resturants! My favorite shows are Friends, How I Met Your Mother, funny sit coms like that. I’m super excited to learm more about LLMs and AI in this class!\n\n(me when i understand how to use github and face no technical difficulties)"
  },
  {
    "objectID": "posts/003_professor_image/odonnell.html",
    "href": "posts/003_professor_image/odonnell.html",
    "title": "Testing Image Analysis",
    "section": "",
    "text": "I hope you are feeling better Professor O’Donnell when you read this and are recovering well! I tried some different prompts running the below pic which was provided on Canvas through the image-to-text API call in Jupyterhub. I was surprised to find some of the different responses that were output depending on which model the image was run through."
  },
  {
    "objectID": "posts/003_professor_image/odonnell.html#the-prompt-describe-this-image.",
    "href": "posts/003_professor_image/odonnell.html#the-prompt-describe-this-image.",
    "title": "Testing Image Analysis",
    "section": "The Prompt: “Describe this image.”",
    "text": "The Prompt: “Describe this image.”\n\nResponses:\n\ngpt 4.0: “The image shows a person wearing a neck brace and looking at the camera. They have some visible injuries on their face, including cuts and bruises. Their right hand is bandaged, and they are holding it up with three fingers extended. The background appears to be indoors with a door and some light coming through.”\nanthropic/claude-3.5-sonnet: “This appears to be a selfie taken from a low angle, showing someone wearing what looks like a VR headset or similar head-mounted device. The person is wearing a light-colored shirt and appears to have some substance or marks on their face. There’s also what appears to be a hand with a cloth or towel visible in the upper right corner of the image. The photo seems to be taken indoors, with a light-colored wall or ceiling visible in the background.”\nGoogle: Gemini 2.5 Flash: “This image is a close-up, selfie-style portrait of an adult male who appears to have recently sustained injuries. The man is looking directly at the camera with a neutral expression. His face shows several signs of injury: there are visible scrapes and scabs on his forehead, particularly above his left eyebrow. His nose has noticeable bruising and scabbing. His lips appear swollen, and there are dark scabs or stitches visible on his lower lip and chin area. He is wearing a white and grey cervical collar or neck brace around his neck. His right hand is raised into the upper right portion of the frame, and it is wrapped in a white, textured bandage or gauze, with his fingers slightly spread. He is wearing a light pink t-shirt, and a dark green design resembling pine trees is visible on the lower left side of the shirt. The background is an indoor setting with light-colored (beige/off-white) walls. To the right, part of a white door frame and a window are visible, through which bright natural light is entering. A small, light-colored sensor is attached to the top of the door frame. The ceiling is also visible at the top of the frame.”"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Angelica and I’m a current sophomore at the University of Pennsylvania studying Health & Societies, concentrating in Healthcare Markets and Finance. I’m also minoring in Neuroscience & Healthcare Management, and Data Science and Analytics.\nAngelica Hu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Blog!",
    "section": "",
    "text": "Unmasking: Unmasking AI\n\n\n\nLLMs\n\nPrompting\n\nLogic\n\n[object Object]\n\n\n\nInside Joy Buolamwini’s Great Mind\n\n\n\n\n\nSep 18, 2025\n\n\nAngelica\n\n\n\n\n\n\n\n\n\n\n\n\nThe Birth of ChatGPT\n\n\n\nLLMs\n\nPrompting\n\nLogic\n\n\n\nHow did we get here?\n\n\n\n\n\nSep 15, 2025\n\n\nAngelica\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Image Analysis\n\n\n\nLLMs\n\nPrompting\n\n\n\nFeel Better Professor O’Donnell!\n\n\n\n\n\nSep 9, 2025\n\n\nAngelica\n\n\n\n\n\n\n\n\n\n\n\n\nMy First Blog Post!\n\n\n\nMy Profile\n\n\n\nGet to Know Me\n\n\n\n\n\nSep 3, 2025\n\n\nAngelica\n\n\n\n\n\n\n\n\n\n\n\n\nThe Wolfram Article\n\n\n\nLLMs\n\n\n\nTakeaways and Observations\n\n\n\n\n\nSep 3, 2025\n\n\nAngelica\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nPrompting\n\nLogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  }
]